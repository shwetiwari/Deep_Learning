{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from urllib import request\n",
    "import gzip\n",
    "import pickle\n",
    "import data\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mnist(final=False, flatten=False):\n",
    "    \"\"\"\n",
    "    Load the MNIST data\n",
    "    :param final: If true, return the canonical test/train split. If false, split some validation data from the training\n",
    "       data and keep the test data hidden.\n",
    "    :param flatten:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    if not os.path.isfile('mnist.pkl'):\n",
    "        init()\n",
    "\n",
    "    xtrain, ytrain, xtest, ytest = load()\n",
    "    xtl, xsl = xtrain.shape[0], xtest.shape[0]\n",
    "\n",
    "    if flatten:\n",
    "        xtrain = xtrain.reshape(xtl, -1)\n",
    "        xtest  = xtest.reshape(xsl, -1)\n",
    "\n",
    "    if not final: # return the flattened images\n",
    "        return (xtrain[:-5000], ytrain[:-5000]), (xtrain[-5000:], ytrain[-5000:]), 10\n",
    "\n",
    "    return (xtrain, ytrain), (xtest, ytest), 10\n",
    "\n",
    "# Numpy-only MNIST loader. Courtesy of Hyeonseok Jung\n",
    "# https://github.com/hsjeong5/MNIST-for-Numpy\n",
    "\n",
    "filename = [\n",
    "[\"training_images\",\"train-images-idx3-ubyte.gz\"],\n",
    "[\"test_images\",\"t10k-images-idx3-ubyte.gz\"],\n",
    "[\"training_labels\",\"train-labels-idx1-ubyte.gz\"],\n",
    "[\"test_labels\",\"t10k-labels-idx1-ubyte.gz\"]\n",
    "]\n",
    "\n",
    "def download_mnist():\n",
    "    base_url = \"http://yann.lecun.com/exdb/mnist/\"\n",
    "    for name in filename:\n",
    "        print(\"Downloading \"+name[1]+\"...\")\n",
    "        request.urlretrieve(base_url+name[1], name[1])\n",
    "    print(\"Download complete.\")\n",
    "\n",
    "def save_mnist():\n",
    "    mnist = {}\n",
    "    for name in filename[:2]:\n",
    "        with gzip.open(name[1], 'rb') as f:\n",
    "            mnist[name[0]] = np.frombuffer(f.read(), np.uint8, offset=16).reshape(-1,28*28)\n",
    "    for name in filename[-2:]:\n",
    "        with gzip.open(name[1], 'rb') as f:\n",
    "            mnist[name[0]] = np.frombuffer(f.read(), np.uint8, offset=8)\n",
    "    with open(\"mnist.pkl\", 'wb') as f:\n",
    "        pickle.dump(mnist,f)\n",
    "    print(\"Save complete.\")\n",
    "\n",
    "def init():\n",
    "    download_mnist()\n",
    "    save_mnist()\n",
    "\n",
    "def load():\n",
    "    with open(\"mnist.pkl\",'rb') as f:\n",
    "        mnist = pickle.load(f)\n",
    "    return mnist[\"training_images\"], mnist[\"training_labels\"], mnist[\"test_images\"], mnist[\"test_labels\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "(xtrain, ytrain), (xtest, ytest), digits = load_mnist()\n",
    "\n",
    "xtrain = (xtrain/255).astype('float32')\n",
    "xtest = (xtest/255).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train Set\n",
    "onehot_y = np.zeros((ytrain.size, ytrain.max()+1))\n",
    "onehot_y[np.arange(ytrain.size),ytrain] = 1\n",
    "# Test Set\n",
    "onehot_y_test = np.zeros((ytest.size, ytest.max()+1))\n",
    "onehot_y_test[np.arange(ytest.size),ytest] = 1\n",
    "\n",
    "ytrain = onehot_y\n",
    "ytest = onehot_y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(55000, 784)\n",
      "(55000, 10)\n",
      "(5000, 784)\n",
      "(5000, 10)\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "print(xtrain.shape)\n",
    "print(ytrain.shape)\n",
    "print(xtest.shape)\n",
    "print(ytest.shape)\n",
    "print(digits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'divide': 'ignore', 'over': 'warn', 'under': 'ignore', 'invalid': 'ignore'}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.seterr(divide='ignore', invalid='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepNeuralNetwork():\n",
    "    def __init__(self, xtrain, sizes, epochs=5, l_rate=0.001):\n",
    "        self.sizes = sizes\n",
    "        self.epochs = epochs\n",
    "        self.records = xtrain.shape[0]\n",
    "        self.l_rate = l_rate\n",
    "\n",
    "        # we save all parameters in the neural network in this dictionary\n",
    "        self.params = self.initialization()\n",
    "\n",
    "    def sigmoid(self, x, derivative=False):\n",
    "        if derivative:\n",
    "            return (np.exp(-x))/((np.exp(-x)+1)**2)\n",
    "        return 1/(1 + np.exp(-x))\n",
    "\n",
    "    def softmax(self, x, derivative=False):\n",
    "        # Numerically stable with large exponentials\n",
    "        exps = np.exp(x - x.max())\n",
    "        if derivative:\n",
    "            return exps / np.sum(exps, axis=0) * (1 - exps / np.sum(exps, axis=0))\n",
    "        return exps / np.sum(exps, axis=0)\n",
    "\n",
    "    def initialization(self):\n",
    "        # number of nodes in each layer\n",
    "        input_layer=self.sizes[0]\n",
    "        hidden_1=self.sizes[1]\n",
    "        output_layer=self.sizes[2]\n",
    "        np.random.seed(2)\n",
    "\n",
    "        params = { \n",
    "            'W1':np.random.randn(input_layer,hidden_1) * np.sqrt(1. / hidden_1),\n",
    "            'W2':np.random.randn(hidden_1,output_layer) * np.sqrt(1. / output_layer),\n",
    "            'b1': np.zeros((1,300)),\n",
    "            'b2': np.zeros((1,10))\n",
    "        }\n",
    "        \n",
    "        return params\n",
    "\n",
    "    def forward_pass(self, x_train):\n",
    "        params = self.params\n",
    "        \n",
    "        W1 = params[\"W1\"]\n",
    "        b1 = params[\"b1\"]\n",
    "        W2 = params[\"W2\"]\n",
    "        b2 = params[\"b2\"]\n",
    "\n",
    "\n",
    "        # input layer activations becomes sample\n",
    "        params['A0'] = x_train\n",
    "        # input layer to hidden layer 1\n",
    "        params['Z1'] = np.dot(W1.T,params['A0']) + params['b1']\n",
    "        params['A1'] = self.sigmoid(params['Z1'])\n",
    "        # hidden layer 1 to output layer\n",
    "        params['Z2'] = np.dot(params['A1'],params[\"W2\"]) + params['b2']\n",
    "        params['A2'] = self.softmax(params['Z2'])\n",
    "\n",
    "        return params['A2']\n",
    "\n",
    "    def backward_pass(self, y_train, output):\n",
    "           \n",
    "        params = self.params\n",
    "        change_w = {}\n",
    "        \n",
    "        #print(params.keys())\n",
    "        A2 = params['A2']\n",
    "        A1 = params['A1']\n",
    "        A0 = params['A0']\n",
    "        Z2 = params['Z2']\n",
    "        Z1 = params['Z1']\n",
    "        W2 = params['W2']\n",
    "        W1 = params['W1']\n",
    "        b2 = params['b2']\n",
    "        b1 = params['b1']\n",
    "        \n",
    "        \n",
    "        dZ2 = A2-y_train\n",
    "        dW2 = dZ2 * A1.T\n",
    "        db2 = np.sum(dZ2, axis = 1, keepdims = True)\n",
    "        dZ1 = np.dot(dZ2, W2.T) * (A1*(1-A1))\n",
    "        dW1 = dZ1.T * A0.T\n",
    "        dW1 = dW1.T\n",
    "        db1 = np.sum(dZ1, axis = 1, keepdims = True)\n",
    "        \n",
    "        change_w['dZ2'] = dZ2\n",
    "        change_w['dW2'] = dW2\n",
    "        change_w['db2'] = db2\n",
    "        change_w['dZ1'] = dZ1\n",
    "        change_w['dW1'] = dW1\n",
    "        change_w['db1'] = db1\n",
    "\n",
    "        return change_w\n",
    "\n",
    "    def update_network_parameters(self, changes_to_w):\n",
    "        \n",
    "        W1 = self.params[\"W1\"]\n",
    "        b1 = self.params[\"b1\"]\n",
    "        W2 = self.params[\"W2\"]\n",
    "        b2 = self.params[\"b2\"]\n",
    "    \n",
    "        dW1 = changes_to_w[\"dW1\"]\n",
    "        db1 = changes_to_w[\"db1\"]\n",
    "        dW2 = changes_to_w[\"dW2\"]\n",
    "        db2 = changes_to_w[\"db2\"]\n",
    "\n",
    "        W1 = W1 - dW1 * self.l_rate\n",
    "        b1 = b1 - db1 * self.l_rate\n",
    "        W2 = W2 - dW2 * self.l_rate\n",
    "        b2 = b2 - db2 * self.l_rate\n",
    "        \n",
    "        self.params['W1'] = W1\n",
    "        self.params['W2'] = W2\n",
    "        self.params['b1'] = b1\n",
    "        self.params['b2'] = b2\n",
    "        \n",
    "    \n",
    "    def compute_cost(self, A2, Y):\n",
    "\n",
    "        m = Y.shape[0] # number of example\n",
    "\n",
    "        cost = -1/ m*np.sum(np.multiply(np.log(A2),Y) + np.multiply(np.log(1-A2), (1-Y)))\n",
    "\n",
    "        cost = np.squeeze(cost)    \n",
    "        assert(isinstance(cost, float))\n",
    "\n",
    "        return cost\n",
    "    \n",
    "    \n",
    "    def compute_accuracy(self, x_val, y_val):\n",
    "        \n",
    "        predictions = []\n",
    "\n",
    "        for x, y in zip(x_val, y_val):\n",
    "            output = self.forward_pass(x)\n",
    "            pred = np.argmax(output)\n",
    "            predictions.append(pred == np.argmax(y))\n",
    "        \n",
    "        return np.mean(predictions)\n",
    "    \n",
    "    \n",
    "    def train(self, x_train, y_train, x_val, y_val):\n",
    "        \n",
    "        train_loss = list()\n",
    "        validation_loss = list()\n",
    "        train_accuracy = list()\n",
    "        validation_accurracy =  list()\n",
    "        start_time = time.time()\n",
    "        m = x_train.shape[0]\n",
    "        n = x_val.shape[0]\n",
    "        \n",
    "        for iteration in range(self.epochs):\n",
    "            loss_train = 0\n",
    "            for x,y in zip(x_train, y_train):\n",
    "                output = self.forward_pass(x)\n",
    "                changes_to_w = self.backward_pass(y, output)\n",
    "                self.update_network_parameters(changes_to_w) \n",
    "                loss_train = loss_train + self.compute_cost(output,y)\n",
    "            cost_train = loss_train/m\n",
    "            train_loss.append(cost_train)\n",
    "            \n",
    "            loss_val = 0\n",
    "            for x,y in zip(x_val, y_val):\n",
    "                output = self.forward_pass(x)\n",
    "                changes_to_w = self.backward_pass(y, output)\n",
    "                self.update_network_parameters(changes_to_w) \n",
    "                loss_val = loss_val + self.compute_cost(output,y)   \n",
    "            cost_val = loss_val/n\n",
    "            validation_loss.append(cost_val)\n",
    "            \n",
    "            accuracy = self.compute_accuracy(x_train, y_train)\n",
    "            train_accuracy.append(accuracy*100)\n",
    "            print('Epoch: {0}, Time Spent: {1:.2f}s, Accuracy: {2:.2f}%'.format(\n",
    "                iteration+1, time.time() - start_time, accuracy * 100))\n",
    "            \n",
    "            accuracy = self.compute_accuracy(x_val, y_val)\n",
    "            validation_accurracy.append(accuracy*100)\n",
    "            print('Epoch: {0}, Time Spent: {1:.2f}s, Accuracy: {2:.2f}%'.format(\n",
    "                iteration+1, time.time() - start_time, accuracy * 100))\n",
    "       \n",
    "        return train_loss, validation_loss, train_accuracy, validation_accurracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Time Spent: 172.45s, Accuracy: 9.88%\n",
      "Epoch: 1, Time Spent: 173.16s, Accuracy: 9.78%\n",
      "Epoch: 2, Time Spent: 375.97s, Accuracy: 9.88%\n",
      "Epoch: 2, Time Spent: 376.75s, Accuracy: 9.78%\n"
     ]
    }
   ],
   "source": [
    "dnn = DeepNeuralNetwork(xtrain,sizes=[784, 300, 10], l_rate=0.05)\n",
    "train_loss, valid_loss, train_accuracy, valid_accuracy =  dnn.train(xtrain, ytrain, xtest, ytest)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Code for Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_validation_loss(list_loss_train, list_loss_validation):\n",
    "        # Create count of the number of epochs\n",
    "        epoch_count = range(1, len(list_loss_train) + 1)\n",
    "        \n",
    "        #plotting\n",
    "        plt.plot(epoch_count,list_loss_train,\"-b\", label=\" training\")\n",
    "        plt.plot(epoch_count,list_loss_validation, \"-r\", label=\"validation\")\n",
    "        plt.ylabel('Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.xticks(rotation=60)\n",
    "        plt.title('Loss w.r.t. epoch for learning rate = 0.05')\n",
    "        plt.legend([\"Training\", \"Validation\"], loc =\"upper right\") \n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_validation_accuracy(list_train_accuracy,list_valid_accuracy):\n",
    "        # Create count of the number of epochs\n",
    "        epoch_count = range(1, len(list_train_accuracy) + 1)\n",
    "        \n",
    "        #plotting\n",
    "        plt.plot(epoch_count,list_train_accuracy,\"-b\", label=\" training\")\n",
    "        plt.plot(epoch_count,list_valid_accuracy, \"-r\", label=\"validation\")\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.xticks(rotation=60)\n",
    "        plt.title('Accuracy w.r.t. epoch for learning rate = 0.05')\n",
    "        plt.legend([\"Training\", \"Validation\"], loc =\"upper right\") \n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot for training vs validation  loss per epoch\n",
    "dnn = DeepNeuralNetwork(sizes=[784, 300, 10], l_rate=0.05)\n",
    "train_loss, valid_loss, train_accuracy, valid_accuracy =  dnn.train(xtrain, ytrain, xtest, ytest)\n",
    "training_validation_loss(train_loss,valid_loss)\n",
    "training_validation_accuracy(train_accuracy,valid_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sgd_learning_rates(valid_loss_1,valid_loss_2,valid_loss_3):\n",
    "    epoch_count = range(1, len(valid_loss_1) + 1)\n",
    "    plt.plot(epoch_count,valid_loss_1,\"-b\", label=\" lr_0.05\") \n",
    "    plt.plot(epoch_count,valid_loss_2,\"-r\", label=\" lr_0.01\") \n",
    "    plt.plot(epoch_count,valid_loss_3,\"-g\", label=\" lr_0.0001\") \n",
    "    plt.ylabel('Performance (Loss)')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.xticks(rotation=60)\n",
    "    plt.title('Loss over 5 epoches')\n",
    "    plt.legend([\"lr_0.05\", \"lr_0.01\",\"lr_0.0001\"], loc =\"upper right\") \n",
    "    plt.show()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot for SGD with different learning rates\n",
    "\n",
    "dnn_1 = DeepNeuralNetwork(sizes=[784, 300, 10], l_rate=0.05)\n",
    "dnn_2 = DeepNeuralNetwork(sizes=[784, 300, 10], l_rate=0.01)\n",
    "dnn_3 = DeepNeuralNetwork(sizes=[784, 300, 10], l_rate=0.0001)\n",
    "train_loss_1, valid_loss_1, train_accuracy_1, valid_accuracy_1 = dnn_1.train(xtrain, ytrain, xtest, ytest)\n",
    "train_loss_2, valid_loss_2, train_accuracy_2, valid_accuracy_2 = dnn_2.train(xtrain, ytrain, xtest, ytest)\n",
    "train_loss_3, valid_loss_3, train_accuracy_3, valid_accuracy_3 = dnn_3.train(xtrain, ytrain, xtest, ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd_learning_rates(valid_loss_1,valid_loss_2,valid_loss_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "dnn_1 = DeepNeuralNetwork(sizes=[784, 300, 10], l_rate=0.001)\n",
    "dnn_2 = DeepNeuralNetwork(sizes=[784, 300, 10], l_rate=0.001)\n",
    "dnn_3 = DeepNeuralNetwork(sizes=[784, 300, 10], l_rate=0.001)\n",
    "train_loss_1, valid_loss_1, train_accuracy_1, valid_accuracy_1 = dnn_1.train(xtrain, ytrain, xtest, ytest)\n",
    "train_loss_2, valid_loss_2, train_accuracy_2, valid_accuracy_2 = dnn_2.train(xtrain, ytrain, xtest, ytest)\n",
    "train_loss_3, valid_loss_3, train_accuracy_3, valid_accuracy_3 = dnn_3.train(xtrain, ytrain, xtest, ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statistics\n",
    "# Plot with mean and std. deviation\n",
    "epoch_count = range(1, len(valid_loss_1) + 1)\n",
    "means= list()\n",
    "std_deviation = list()\n",
    "temp = list()\n",
    "\n",
    "for i in range(len(epoch_count)):\n",
    "    temp.append(valid_loss_1[i])\n",
    "    temp.append(valid_loss_2[i])\n",
    "    temp.append(valid_loss_3[i])\n",
    "    means.append(statistics.mean(temp))\n",
    "\n",
    "\n",
    "for i in range(len(epoch_count)):\n",
    "    temp.append(valid_loss_1[i])\n",
    "    temp.append(valid_loss_2[i])\n",
    "    temp.append(valid_loss_3[i])\n",
    "    std_deviation.append(statistics.stdev(temp))\n",
    "\n",
    "\n",
    "# plot it!\n",
    "fig, ax = plt.subplots(1)\n",
    "ax.plot(epoch_count, means + std_deviation , lw=2, label='mean+SGD', color='blue')\n",
    "ax.plot(epoch_count, means - std_deviation , lw=2, label='mean-SGD', color='orange')\n",
    "ax.plot(epoch_count, means , lw=2, label='green', color='blue')\n",
    "ax.set_title('Average and Std. deviation of Loss')\n",
    "ax.legend(loc='upper left')\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Loss')\n",
    "ax.grid()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
