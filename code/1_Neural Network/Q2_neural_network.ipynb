{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning - Assignment 1\n",
    "\n",
    "- Build a neural network from scratch\n",
    "- Implement a 2-class classification neural network with a single hidden layer\n",
    "- Use sigmoid activation function at hidden layer and softmax at output layer\n",
    "- Compute the cross entropy loss as this is a classification task\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import math\n",
    "from math import log\n",
    "import numpy as np\n",
    "from random import random "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1 - Defining the neural network structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = ([1.,-1.]) # Input Vector\n",
    "Y = ([1., 0.])  # Output Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + math.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(X):\n",
    "    numerator = np.exp(X)\n",
    "    denominator = np.sum(np.exp(X))\n",
    "    return numerator/denominator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorical_cross_entropy(predicted, actual):\n",
    "    sum_score = 0.0\n",
    "    for i in range(len(actual)):\n",
    "        sum_score += actual[i] * log(1e-15 + predicted[i])\n",
    "    mean_sum_score = 1.0 / len(actual) * sum_score\n",
    "    return -2 * mean_sum_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2 - Initialize the model's parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters():\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "    params -- python dictionary containing parameters:\n",
    "                  \n",
    "    \"\"\"  \n",
    "    W = ([[1., 1., 1.], [-1., -1., -1.]])\n",
    "    b = ([0., 0., 0.])\n",
    "    V = ([[1., 1.], [-1., -1.],[-1., -1.]])\n",
    "    c = ([0., 0.])\n",
    "    k1 = ([0., 0., 0.])\n",
    "    h = ([0., 0., 0.])\n",
    "    k2 = ([0., 0.])\n",
    "    y = ([0., 0.])\n",
    "    \n",
    "    parameters = {\"W\": W,\n",
    "                  \"b\": b,\n",
    "                  \"V\": V,\n",
    "                  \"c\": c,\n",
    "                  \"k1\": k1,\n",
    "                  \"h\": h,\n",
    "                  \"k2\": k2,\n",
    "                  \"y\": y}\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W = [[1.0, 1.0, 1.0], [-1.0, -1.0, -1.0]]\n",
      "b = [0.0, 0.0, 0.0]\n",
      "V = [[1.0, 1.0], [-1.0, -1.0], [-1.0, -1.0]]\n",
      "c = [0.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "parameters = initialize_parameters()\n",
    "print(\"W = \" + str(parameters[\"W\"]))\n",
    "print(\"b = \" + str(parameters[\"b\"]))\n",
    "print(\"V = \" + str(parameters[\"V\"]))\n",
    "print(\"c = \" + str(parameters[\"c\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3 - Implement forward_propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(X, parameters):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "    y -- The softmax output of the second activation\n",
    "    cache -- a dictionary containing \"k1\", \"h\", \"k2\" and \"y\"\n",
    "    \"\"\"\n",
    "    # Retrieve each parameter from the dictionary \"parameters\"\n",
    "    W = parameters[\"W\"]\n",
    "    b = parameters[\"b\"]\n",
    "    V = parameters[\"V\"]\n",
    "    c = parameters[\"c\"]\n",
    "    k1 = parameters[\"k1\"]\n",
    "    h = parameters[\"h\"]\n",
    "    k2 = parameters[\"k2\"]\n",
    "    y = parameters[\"y\"]\n",
    "   \n",
    "    # Implement Forward Propagation to calculate y (output) (probabilities)\n",
    "    transpose_W = list(map(list, zip(*W)))\n",
    "    transpose_V = list(map(list, zip(*V)))\n",
    "    \n",
    "    for j in range(len(transpose_W)):\n",
    "        for i in range(len(X)):\n",
    "            k1[j] += W[i][j] * X[i]\n",
    "        k1[j] += b[j]\n",
    "    print(f\"k1 = {k1}\")\n",
    "    \n",
    "    for i in range(len(k1)):\n",
    "        h[i] = sigmoid(k1[i])\n",
    "    print(f\"h = {h}\")\n",
    "    \n",
    "    for j in range(len(transpose_V)):\n",
    "        for i in range(len(h)):\n",
    "            k2[j] += V[i][j] * h[i]\n",
    "        k2[j] += c[j]\n",
    "    print(f\"k2 = {k2}\")\n",
    "    \n",
    "    y = softmax(k2)\n",
    "    print(f\"y = {y}\")\n",
    "    \n",
    "    cache = {\"k1\": k1,\n",
    "             \"h\": h,\n",
    "             \"k2\": k2,\n",
    "             \"y\": y}\n",
    "    \n",
    "    return y, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k1 = [2.0, 2.0, 2.0]\n",
      "h = [0.8807970779778823, 0.8807970779778823, 0.8807970779778823]\n",
      "k2 = [-0.8807970779778823, -0.8807970779778823]\n",
      "y = [0.5 0.5]\n"
     ]
    }
   ],
   "source": [
    "y, cache = forward_propagation(X, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost = 0.6931471805599433\n"
     ]
    }
   ],
   "source": [
    "print(\"cost = \" + str(categorical_cross_entropy(y, Y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_propagation(parameters, cache ,X, Y):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "    grads -- python dictionary containing gradients with respect to different parameters\n",
    "    \"\"\"\n",
    "    # First, W and V and retrived from the dictionary \"parameters\".\n",
    "    W = parameters[\"W\"]\n",
    "    b = parameters[\"b\"]\n",
    "    V = parameters[\"V\"]\n",
    "    c = parameters[\"c\"]\n",
    "    \n",
    "    transpose_W = list(map(list, zip(*W)))\n",
    "    transpose_V = list(map(list, zip(*V)))\n",
    "        \n",
    "    # Retrieved also k1 and k2 from dictionary \"cache\".\n",
    "    k1 = cache[\"k1\"]\n",
    "    h = cache[\"h\"]\n",
    "    k2 = cache[\"k2\"]  \n",
    "    y = cache[\"y\"]\n",
    "    \n",
    "    dW = ([[0., 0., 0.], [0., 0., 0.]])\n",
    "    dk1 = ([0., 0., 0.])\n",
    "    db = ([0., 0., 0.])\n",
    "    dh = ([0., 0., 0.])\n",
    "    dV = ([[0., 0.], [0., 0.],[0., 0.]])\n",
    "    dk2 = ([0., 0.])\n",
    "    dc = ([0., 0.])\n",
    "    \n",
    "    dk2 = y - Y\n",
    "    \n",
    "    for i in range(len(transpose_W)):\n",
    "        for j in range(len(k2)):\n",
    "            dV[i][j] = dk2[j] * h[i]\n",
    "            dh[i] += dk2[j] * V[i][j]\n",
    "    \n",
    "    dc = dk2\n",
    "    \n",
    "    for i in range(len(transpose_W)):\n",
    "        dk1[i] = dh[i] * h[i] * (1 - h[i])\n",
    "    \n",
    "    for j in range(len(transpose_W)):\n",
    "        for i in range(len(X)):\n",
    "            dW[i][j] = dk1[j] * X[i]\n",
    "        db[j] = dk1[j]\n",
    "\n",
    "    \n",
    "    grads = {\"dW\": dW,\n",
    "             \"db\": db,\n",
    "             \"dV\": dV,\n",
    "             \"dc\": dc}\n",
    "\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dW = [[0.0, 0.0, 0.0], [-0.0, -0.0, -0.0]]\n",
      "db = [0.0, 0.0, 0.0]\n",
      "dV = [[-0.44039853898894116, 0.44039853898894116], [-0.44039853898894116, 0.44039853898894116], [-0.44039853898894116, 0.44039853898894116]]\n",
      "dc = [-0.5  0.5]\n"
     ]
    }
   ],
   "source": [
    "grads = backward_propagation(parameters, cache, X, Y)\n",
    "print (\"dW = \"+ str(grads[\"dW\"]))\n",
    "print (\"db = \"+ str(grads[\"db\"]))\n",
    "print (\"dV = \"+ str(grads[\"dV\"]))\n",
    "print (\"dc = \"+ str(grads[\"dc\"]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
